---
title: "dHM2013"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{dHM2013}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# library(dHM2013R)
load_all()

library(data.table)
library(magrittr)

```

This vignette documents my attempt to replicate the analysis of @dhaultfoeuille_inference_2013 on British data, specifically the longitudinal cohort studies Next Steps and BCS 1970.

Their analysis is based upon the extended Roy model, with two sectors, non-pecuniary factors and uncertainty in future earnings.
$$
Y_0 = X'\beta_0 + \varepsilon_0 \\
Y_1 = X'\beta_1 + \varepsilon_1 \\
D = 1\{ -\delta_0 + X'(\beta_1 - \beta_0 - \gamma_0) + \eta_{\Delta} > 0 \}
$$
where $\varepsilon_k = \eta_k + \nu_k$, $\eta_{\Delta} = \eta_1 - \eta_0$, and $X \perp (\eta_1, \eta_0)$. The young person knows $\eta_k$ at the time they make their decision but not $\nu_k$.

@dhaultfoeuille_inference_2013's method is a 3-step procedure that relies on $\eta_k$ linking the outcome ($y_k$) and choice ($D$) equations. The 3 steps are as follows:
1) estimate $\zeta_0 = \frac{\beta_1 - \beta_0 - \gamma_0}{\beta_{11} - \beta_{01} - \gamma_{01}} \Rightarrow \zeta_{01} = 1$, where $x_1$ is some continuous $x \in X$. Use @coppejans_estimation_2001 approach. 
2) estimate $\beta_1$ and $\beta_0$ via @newey_two-step_2009's series estimator (which involves using the index from step-1 and its power series as a control function).
3) estimate ($\delta_0$ and $\gamma_0$) via @dhaultfoeuille_inference_2013's novel method.

# Step 1 (we could also use a logit or probit here) 

Estimating $\zeta$.

## Probit and logit estimation for comparison

```{r logit}

benefitVars <- c(
  'betterJob', 'betterPay', 'betterOpp', 'need4Career', 'showSkills',
  'delayWorkGd', 'socialLifeGd', 'leaveHomeGd', 'keepStudy', 'moreQuals',
  'persDevel', 'moreConfid', 'moreRespect', 'betterLife', 'prepareLife'
)

costVars <- c(
  'tooExpensive', 'getDebt', 'parentsMoney', 'notFinancIndep', 'notWorkEarn',
  'costsGen', 'noGuaranteeGdJb', 'notNeed4Career', 'lessExp', 'tooHard',
  'leaveHomeBad', 'tooLong', 'wasteTime', 'feesEtc', 'stress'
)

xControls <- c(
  'w4SOCMajorMP', 'w4ethgrpMP',
  'w4hiqualgMP', 'W5AlevNoYP', 'W5EMA1YP',
  'W4AlevNoYP', 'w4sexMP', 'W4SexYP',
  "cogScore"
)

# regression predicted wages
logitControlsFit <- c(benefitVars, costVars, xControls)

step1Formula <- as.formula(
  paste0('degree ~ ', paste0(logitControlsFit, collapse = ' + '))
)

step1Probit <- glm(
  step1Formula,
  data = dtLsype4dHM,
  na.action = na.exclude
)

summary(step1Probit)

step1Logit <- glm(
  step1Formula,
  data = dtLsype4dHM,
  family = binomial(),
  na.action = na.exclude
)

summary(step1Logit)
```

## Mixture of distributions (MOD) estimator (@coppejans_estimation_2001)

@coppejans_estimation_2001 proposes the mixture of distributions estimator, that allows estimation of binary response models when the error distribution is unknown. Rather than assume a logistic or Gaussian error distribution, the MOD estimator employs a mixture of smooth distributions. The method relies upon a single index as we have in the extended Roy model here. 

The estimator maximises
$$
Q_n(\omega(x;\Psi_k, \zeta)) = \frac{1}{n}\sum_{i=1}^n \{y_i - [1 - \Psi_k(-x'\zeta; a, b, \{\mu_j, \pi_j\}, \sigma_k)] \}^2
$$
with respect to $(\zeta, a, b, \{\mu_j, \pi_j\}, \sigma_k)$, and where
$$
\Psi_k(u; a, b, \{\mu_j, \pi_j\}, \sigma_k)  = a - (b-a)\sum_{j=1}^k \pi_j H\left( \frac{u - \mu_j}{\sigma_k} \right).
$$
Here, $k$ is the number of mixing components and $H(\cdot)$ is some smooth distribution (e.g. $\Phi(\cdot)$), $\pi_j, 0 \leq \pi_j \leq 1, \sum_{j=1}^k \pi_j = 1$ are mixing weights, $\mu_j$ is a location parameter, $a$ is an intercept and $b-a$ a slope term (we can set $b=1$ and $a=0$ here).

The maximisation is subject to the following constraints:

- $\sigma_k > \min\{0.5, \frac{0.5\hat{\sigma}_{p}}{n^{0.2}}\}$, where $\hat{\sigma}_{p}$ is the Probit estimate of $\sigma$
- an upper bound on the second derivative:
  $$
  \left| \frac{b-a}{\sigma^2_k}\sum_{j=1}^k \pi_j H^{(2)}\left( \frac{z_{j,l} - \mu_j}{\sigma_k} \right) \right| \leq C^{(2)}_{\Psi}, \,l = 1,2,
  $$
  where $z_{j,1} = \mu_j - \sigma_k$ and $z_{j,1} = \mu_j + \sigma_k$, and
  $$
  C^{(2)}_{\Psi} \equiv \min\left\{ \frac{\exp{(-0.5)}}{\sqrt{2\pi}}\max\left[ 0.5^{-2}, \left( \frac{0.5\hat{\sigma_p}}{n^{0.2}} \right)^{-1.5} \right], 1000 \right\}.
  $$
  This constraint is motivated by noting that $\left| H^{(2)}\left( \frac{z - \mu_j}{\sigma_k} \right) \right|$ obtains a maximum at $\mu_j \pm \sigma_k$. 

However, for computation we only impose the bound on $\sigma$ and will check the bounds on the derivative after maximising.

@coppejans_estimation_2001 suggests picking $k = 7$ for 250 observations, though @dhaultfoeuille_inference_2013 use $k=3$.   

```{r constraints from probit}

sigma_p <- sd(step1Probit$residuals)

sigma_lb <- min(.5 * sigma_p / (1593^.2), 0.5)

c2Psi_ub <- min((exp(-.5)/sqrt(2*pi)) * max(1/4, (.5 * sigma_p / (1593^.2))^(-1.5)), 1000)

```

```{r step1 coppejans}

k <- 3

pi_k <- rep(1/k, k-1)

mu <- rep(0, k)
sigma <- sigma_p

zeta <- step1Probit$coefficients[2:59] / step1Probit$coefficients[[59]]

x <- model.matrix(
  object = step1Probit$formula,
  data = step1Probit$model
)[, 2:59]

x_x1 <- x / x[, "cogScore"]
  
y <- step1Probit$y

y_x1 <- y / x[, "cogScore"]

Q <- function(zeta, pik, mu, sigma, x, y) {
  mean((y - (1 - (
    pik[[1]] * pnorm(- x %*% zeta, mean = mu[[1]], sd = sigma) +
      pik[[2]] * pnorm(- x %*% zeta, mean = mu[[2]], sd = sigma) +
      (1 - pik[[1]] - pik[[2]]) * pnorm(- x %*% zeta, mean = mu[[3]], sd = sigma)
  )))^2)
}

Q2 <- function(theta) {
  Q(zeta = c(theta[1:57], 1), pik = theta[58:59], mu = theta[60:62], 
    sigma = theta[63], x = x, y = y)
}

step1Coppejans <- optim(
  par = c(zeta[1:57], pi_k, mu, sigma),
  fn = Q2, method = "BFGS", control = list(maxit = 500)
)

```

```{r compare probit logit and coppejans}

step1Estimates <- data.table(
  varName = names(step1Probit$coefficients), 
  probitEstimate = step1Probit$coefficients / step1Probit$coefficients["cogScore"]
) %>% 
  merge(
    data.table(
      varName = names(step1Logit$coefficients), 
      logitEstimate = step1Logit$coefficients / step1Logit$coefficients["cogScore"]
    ), by = "varName"
) %>%
  merge(
    data.table(
      varName = names(step1Coppejans$par[1:58]), 
      coppejansEstimate = step1Coppejans$par[1:58] / step1Coppejans$par[[58]]
    ), by = "varName"
)


```

Comparing the parameter estimates across Probit, Logit and @coppejans_estimation_2001 approaches, the signs of the estimates are all the same---except the intercept which is positive for Probit and negative for the other two. 

# Step 2 (@newey_two-step_2009)

The idea behind step 2 is to use a control function approach *a la* @heckman_common_1976 but replacing the inverse Mills ratio with a function of the index from step 1. 

A possibility suggested in @newey_two-step_2009 is to use a power series of the index itself, so if we borrow the notation from that paper and let $\hat{v} = x'\hat{\zeta}$ then the control function is $h(\hat{v}) = \sum_{k=1}^K \hat{v}^k$.

We can then estimate $(\beta_1, \beta_0)$ directly using least squares in regressions of the form:
$$
y_1 = x\beta_1 + h(\hat{v}) \\
y_0 = x\beta_0 + h(1-\hat{v})
$$
Not completely sure about the second equation here---is that the correct control function? 

```{r step 2}

vhatVec <- x %*% c(step1Coppejans$par[1:57], 1)

dtLsype4dHM <- dtLsype4dHM[, vhat1 := vhatVec[, 1]]
dtLsype4dHM[, one_vhat1 := 1 - vhat1]
dtLsype4dHM[, paste0("vhat", 2:6) := .(vhat1^2, vhat1^3, vhat1^4, vhat1^5, vhat1^6)]
dtLsype4dHM[, paste0("one_vhat", 2:6) := .(one_vhat1^2, one_vhat1^3, one_vhat1^4, one_vhat1^5, one_vhat1^6)]

step2FormulaD1 <- as.formula(
  paste0('logHourWage ~ ', paste0(c(xControls, "betterPay", paste0("vhat", 1:6)), collapse = ' + '))
)

step2FormulaD0 <- as.formula(
  paste0('logHourWage ~ ', paste0(c(xControls, "betterPay", paste0("one_vhat", 1:6)), collapse = ' + '))
)

step2Formula_noVhat <- as.formula(
  paste0('logHourWage ~ ', paste0(c(xControls, "betterPay"), collapse = ' + '))
)

step2d1 <- lm(
  formula = step2FormulaD1,
  data = dtLsype4dHM[degree == TRUE]
)

step2d1_noVhat <- lm(
  formula = step2Formula_noVhat,
  data = dtLsype4dHM[degree == TRUE]
)

step2d0 <- lm(
  formula = step2FormulaD0,
  data = dtLsype4dHM[degree == FALSE]
)

step2d0_noVhat <- lm(
  formula = step2Formula_noVhat,
  data = dtLsype4dHM[degree == FALSE]
)

step2Estimates <- data.table(
  varName = names(step2d1$coefficients),
  d1estimates = step2d1$coefficients
) %>%
  merge(
    data.table(
      varName = names(step2d1_noVhat$coefficients),
      d1estimates_noVhat = step2d1_noVhat$coefficients
    ), by = "varName"
  ) %>%
  merge(
    data.table(
      varName = names(step2d0$coefficients),
      d0estimates = step2d0$coefficients
    ), by = "varName"
  ) %>%
  merge(
    data.table(
      varName = names(step2d0_noVhat$coefficients),
      d0estimates_noVhat = step2d0_noVhat$coefficients
    ), by = "varName"
  )
```
